{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13DgVy7gNpiw-yS7H6stE11x6cD09Bl7P","authorship_tag":"ABX9TyPsUzQ2pe/L8QnEkcy+vtb0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/GAN_image_colorizing-master')"],"metadata":{"id":"IgPkCV_CBX5a","executionInfo":{"status":"ok","timestamp":1670425527372,"user_tz":-420,"elapsed":4,"user":{"displayName":"Thịnh Nguyễn","userId":"12191653371483896772"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"7QmTZC7ABFww","executionInfo":{"status":"ok","timestamp":1670426423659,"user_tz":-420,"elapsed":987,"user":{"displayName":"Thịnh Nguyễn","userId":"12191653371483896772"}}},"outputs":[],"source":["import os\n","import argparse\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from datasets import Cifar10Dataset\n","from networks import Generator, Discriminator, weights_init_normal\n","from helpers import print_args, print_losses\n","from helpers import save_sample, adjust_learning_rate\n","\n","\n","def init_training(data_path='./data', batch_size, num_workers, gen_norm, disc_norm, apply_weight_init,\n","                  save_path, start_epoch, max_epoch, lr_decay_rate,\n","                  base_lr_gen, lr_decay_steps, base_lr_disc,l1_weight,smoothing,save_freq):\n","    \"\"\"Initialize the data loader, the networks, the optimizers and the loss functions.\"\"\"\n","    datasets = Cifar10Dataset.get_datasets_from_scratch(data_path)\n","    for phase in ['train', 'test']:\n","        print('{} dataset len: {}'.format(phase, len(datasets[phase])))\n","\n","    # define loaders\n","    data_loaders = {\n","        'train': DataLoader(datasets['train'], batch_size=batch_size, shuffle=True, num_workers=num_workers),\n","        'test': DataLoader(datasets['test'], batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","    }\n","\n","    # check CUDA availability and set device\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print('Use GPU: {}'.format(str(device) != 'cpu'))\n","\n","    # set up models\n","    generator = Generator(gen_norm).to(device)\n","    discriminator = Discriminator(disc_norm).to(device)\n","\n","    # initialize weights\n","    if apply_weight_init:\n","        generator.apply(weights_init_normal)\n","        discriminator.apply(weights_init_normal)\n","\n","    # adam optimizer with reduced momentum\n","    optimizers = {\n","        'gen': torch.optim.Adam(generator.parameters(), lr=base_lr_gen, betas=(0.5, 0.999)),\n","        'disc': torch.optim.Adam(discriminator.parameters(), lr=base_lr_disc, betas=(0.5, 0.999))\n","    }\n","\n","    # losses\n","    losses = {\n","        'l1': torch.nn.L1Loss(reduction='mean'),\n","        'disc': torch.nn.BCELoss(reduction='mean')\n","    }\n","\n","    # make save dir, if it does not exists\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","\n","    # load weights if the training is not starting from the beginning\n","    global_step = start_epoch * len(data_loaders['train']) if start_epoch > 0 else 0\n","    if start_epoch > 0:\n","\n","        generator.load_state_dict(torch.load(\n","            os.path.join(save_path, 'checkpoint_ep{}_gen.pt'.format(start_epoch - 1)),\n","            map_location=device\n","        ))\n","        discriminator.load_state_dict(torch.load(\n","            os.path.join(save_path, 'checkpoint_ep{}_disc.pt'.format(start_epoch - 1)),\n","            map_location=device\n","        ))\n","\n","    return global_step, device, data_loaders, generator, discriminator, optimizers, losses\n","\n","\n","def run_training(data_path, batch_size, num_workers, gen_norm, disc_norm,\n","                 apply_weight_init, base_lr_gen, base_lr_disc,\n","                 save_path, start_epoch, max_epoch, lr_decay_rate,\n","                 lr_decay_steps,l1_weight, smoothing,save_freq):\n","    \"\"\"Initialize and run the training process.\"\"\"\n","    global_step, device, data_loaders, generator, discriminator, optimizers, losses = init_training(data_path, batch_size, num_workers, gen_norm,#\n","    disc_norm, apply_weight_init, base_lr_gen, base_lr_disc, save_path, start_epoch, max_epoch, lr_decay_rate, lr_decay_steps,l1_weight,smoothing,save_freq)\n","    #  run training process\n","    for epoch in range(start_epoch, max_epoch):\n","        print('\\n========== EPOCH {} =========='.format(epoch))\n","\n","        for phase in ['train', 'test']:\n","\n","            # running losses for generator\n","            epoch_gen_adv_loss = 0.0\n","            epoch_gen_l1_loss = 0.0\n","\n","            # running losses for discriminator\n","            epoch_disc_real_loss = 0.0\n","            epoch_disc_fake_loss = 0.0\n","            epoch_disc_real_acc = 0.0\n","            epoch_disc_fake_acc = 0.0\n","\n","            if phase == 'train':\n","                print('TRAINING:')\n","            else:\n","                print('VALIDATION:')\n","\n","            for idx, sample in enumerate(data_loaders[phase]):\n","\n","                # get data\n","                img_l, real_img_lab = sample[:, 0:1, :, :].float().to(device), sample.float().to(device)\n","\n","                # generate targets\n","                target_ones = torch.ones(real_img_lab.size(0), 1).to(device)\n","                target_zeros = torch.zeros(real_img_lab.size(0), 1).to(device)\n","\n","                if phase == 'train':\n","                    # adjust LR\n","                    global_step += 1\n","                    adjust_learning_rate(optimizers['gen'], global_step, base_lr=base_lr_gen,\n","                                         lr_decay_rate=lr_decay_rate, lr_decay_steps=lr_decay_steps)\n","                    adjust_learning_rate(optimizers['disc'], global_step, base_lr=base_lr_disc,\n","                                         lr_decay_rate=lr_decay_rate, lr_decay_steps=lr_decay_steps)\n","\n","                    # reset generator gradients\n","                    optimizers['gen'].zero_grad()\n","\n","                # train / inference the generator\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    fake_img_ab = generator(img_l)\n","                    fake_img_lab = torch.cat([img_l, fake_img_ab], dim=1).to(device)\n","\n","                    # adv loss\n","                    adv_loss = losses['disc'](discriminator(fake_img_lab), target_ones)\n","                    # l1 loss\n","                    l1_loss = losses['l1'](real_img_lab[:, 1:, :, :], fake_img_ab)\n","                    # full gen loss\n","                    full_gen_loss = (1.0 - l1_weight) * adv_loss + (l1_weight * l1_loss)\n","\n","                    if phase == 'train':\n","                        full_gen_loss.backward()\n","                        optimizers['gen'].step()\n","\n","                epoch_gen_adv_loss += adv_loss.item()\n","                epoch_gen_l1_loss += l1_loss.item()\n","\n","                if phase == 'train':\n","                    # reset discriminator gradients\n","                    optimizers['disc'].zero_grad()\n","\n","                # train / inference the discriminator\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    prediction_real = discriminator(real_img_lab)\n","                    prediction_fake = discriminator(fake_img_lab.detach())\n","\n","                    loss_real = losses['disc'](prediction_real, target_ones * smoothing)\n","                    loss_fake = losses['disc'](prediction_fake, target_zeros)\n","                    full_disc_loss = loss_real + loss_fake\n","\n","                    if phase == 'train':\n","                        full_disc_loss.backward()\n","                        optimizers['disc'].step()\n","\n","                epoch_disc_real_loss += loss_real.item()\n","                epoch_disc_fake_loss += loss_fake.item()\n","                epoch_disc_real_acc += np.mean(prediction_real.detach().cpu().numpy() > 0.5)\n","                epoch_disc_fake_acc += np.mean(prediction_fake.detach().cpu().numpy() <= 0.5)\n","\n","                # save the first sample for later\n","                if phase == 'test' and idx == 0:\n","                    sample_real_img_lab = real_img_lab\n","                    sample_fake_img_lab = fake_img_lab\n","\n","            # display losses\n","            print_losses(epoch_gen_adv_loss, epoch_gen_l1_loss,\n","                         epoch_disc_real_loss, epoch_disc_fake_loss,\n","                         epoch_disc_real_acc, epoch_disc_fake_acc,\n","                         len(data_loaders[phase]), l1_weight)\n","\n","            # save after every nth epoch\n","            if phase == 'test':\n","                if epoch % save_freq == 0 or epoch == max_epoch - 1:\n","                    gen_path = os.path.join(save_path, 'checkpoint_ep{}_gen.pt'.format(epoch))\n","                    disc_path = os.path.join(save_path, 'checkpoint_ep{}_disc.pt'.format(epoch))\n","                    torch.save(generator.state_dict(), gen_path)\n","                    torch.save(discriminator.state_dict(), disc_path)\n","                    print('Checkpoint.')\n","\n","                # display sample images\n","                save_sample(\n","                    sample_real_img_lab,\n","                    sample_fake_img_lab,\n","                    os.path.join(save_path, 'sample_ep{}.png'.format(epoch))\n","                )\n","\n","\n","# def get_arguments():\n","#     \"\"\"Get command line arguments.\"\"\"\n","#     parser = argparse.ArgumentParser(\n","#         description='Image colorization with GANs',\n","#         formatter_class=argparse.ArgumentDefaultsHelpFormatter\n","#     )\n","#     parser.add_argument('--data_path', type=str, default='./data',\n","#                         help='Download and extraction path for the dataset.')\n","#     parser.add_argument('--save_path', type=str, default='./checkpoints',\n","#                         help='Save and load path for the network weights.')\n","#     parser.add_argument('--save_freq', type=int, default=5, help='Save frequency during training.')\n","#     parser.add_argument('--batch_size', type=int, default=32)\n","#     parser.add_argument('--num_workers', type=int, default=4)\n","#     parser.add_argument('--start_epoch', type=int, default=0,\n","#                         help='If start_epoch>0, load previously saved weigth from the save_path.')\n","#     parser.add_argument('--max_epoch', type=int, default=200)\n","#     parser.add_argument('--smoothing', type=float, default=0.9)\n","#     parser.add_argument('--l1_weight', type=float, default=0.99)\n","#     parser.add_argument('--base_lr_gen', type=float, default=3e-4, help='Base learning rate for the generator.')\n","#     parser.add_argument('--base_lr_disc', type=float, default=6e-5, help='Base learning rate for the discriminator.')\n","#     parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='Learning rate decay rate for both networks.')\n","#     parser.add_argument('--lr_decay_steps', type=float, default=6e4, help='Learning rate decay steps for both networks.')\n","#     parser.add_argument('--gen_norm', type=str, default='batch', choices=['batch', 'instance'],\n","#                         help='Defines the type of normalization used in the generator.')\n","#     parser.add_argument('--disc_norm', type=str, default='batch', choices=['batch', 'instance', 'spectral'],\n","#                         help='Defines the type of normalization used in the discriminator.')\n","#     parser.add_argument('--apply_weight_init', type=int, default=0, choices=[0, 1],\n","#                         help='If set to 1, applies the \"weights_init_normal\" function from networks.py.')\n","#     return parser.parse_args()\n","\n","\n","# if __name__ == '__main__':\n","#     args = get_arguments()\n","\n","#     # display arguments\n","#     print_args(args)\n","\n","#     run_training(args)"]},{"cell_type":"code","source":["run_training()"],"metadata":{"id":"INxcT4fVBqeJ"},"execution_count":null,"outputs":[]}]}